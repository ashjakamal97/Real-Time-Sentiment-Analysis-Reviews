{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248cc1a5-d5a0-411b-be99-9657a0026b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE 1: scraper.py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import textwrap\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e594c0-176a-4b2c-9416-172b255c3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9746ece-c13e-4df1-8c1a-21ec079600e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Scraping: iPhone 15\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n",
      "\n",
      "🔍 Scraping: Samsung Galaxy A35\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n",
      "\n",
      "🔍 Scraping: Samsung 7kg Washing Machine\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n",
      "\n",
      "🔍 Scraping: Milton Electric Kettle\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n",
      "\n",
      "🔍 Scraping: Realme C61\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n",
      "\n",
      "🔍 Scraping: MarQ AC 2025\n",
      "✅ Page 1 scraped.\n",
      "✅ Page 2 scraped.\n",
      "✅ Page 3 scraped.\n",
      "✅ Page 4 scraped.\n",
      "✅ Page 5 scraped.\n",
      "✅ Page 6 scraped.\n",
      "✅ Page 7 scraped.\n",
      "✅ Page 8 scraped.\n",
      "✅ Page 9 scraped.\n",
      "✅ Page 10 scraped.\n"
     ]
    }
   ],
   "source": [
    "#input url\n",
    "product_urls = {\n",
    "    \"iPhone 15\": \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W\",\n",
    "    \"Samsung Galaxy A35\": \"https://www.flipkart.com/samsung-galaxy-a35-5g-awesome-navy-256-gb/product-reviews/itm2d2e398127998?pid=MOBGYT2HEEYGMZFH\",\n",
    "    \"Samsung 7kg Washing Machine\": \"https://www.flipkart.com/samsung-7-kg-5-star-ecobubble-technology-hygiene-steam-digital-inverter-fully-automatic-front-load-washing-machine-in-built-heater-grey/product-reviews/itm1eb327d8b2ee1?pid=WMNGYGJKCVNKSZWY\",\n",
    "    \"Milton Electric Kettle\": \"https://www.flipkart.com/milton-electro-electric-kettle/product-reviews/itm7071829829f15?pid=EKTG26FTFQSG84CG\",\n",
    "    \"Realme C61\": \"https://www.flipkart.com/realme-c61-safari-green-128-gb/product-reviews/itmd6ddbcefce040?pid=MOBHFRKRAVXUKDGX\",\n",
    "    \"MarQ AC 2025\": \"https://www.flipkart.com/marq-flipkart-2025-1-ton-5-star-split-inverter-5-in-1-convertible-turbo-cool-technology-ac-white/product-reviews/itmfd8dfe14ce4f5?pid=ACNH76Z3Q6TDP42V\"\n",
    "   \n",
    "}\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def fix_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str.strip(), \"%d %b, %Y\").strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        return date_str.strip()\n",
    "\n",
    "MAX_PAGES=10 #input user limit\n",
    "all_reviews = []  # ✅ This line is essential\n",
    "\n",
    "for product, url in product_urls.items():\n",
    "    print(f\"\\n🔍 Scraping: {product}\")\n",
    "    driver.get(url)\n",
    "    page = 0\n",
    "\n",
    "    while page < MAX_PAGES:\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"z9E0IG\")))\n",
    "\n",
    "            authors = driver.find_elements(By.CSS_SELECTOR, \"._2NsDsF.AwS1CA\")\n",
    "            ratings = driver.find_elements(By.CLASS_NAME, \"XQDdHH\")\n",
    "            reviews = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "            dates = driver.find_elements(By.XPATH, \"//p[contains(@class, '_2NsDsF') and not(contains(@class, 'AwS1CA'))]\")\n",
    "\n",
    "            if not reviews:\n",
    "                print(f\"❌ No reviews on page {page + 1}\")\n",
    "                break\n",
    "\n",
    "            for a, r, t, d in zip(authors, ratings, reviews, dates):\n",
    "                all_reviews.append({\n",
    "                    \"Product\": product,\n",
    "                    \"Author\": a.text.strip(),\n",
    "                    \"Rating\": r.text.strip(),\n",
    "                    \"Review\": textwrap.fill(t.text.strip(), width=50),\n",
    "                    \"Date\": fix_date(d.text.strip())\n",
    "                })\n",
    "\n",
    "            print(f\"✅ Page {page + 1} scraped.\")\n",
    "            page += 1\n",
    "\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, \"//span[text()='Next']\")\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                print(f\"⏹ No Next button or error: {e}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error on {product}, page {page + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f3257e-89c2-4a3d-8820-13cda0759ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def fix_date(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    x = x.strip()\n",
    "\n",
    "    # Already formatted like \"Oct, 2023\"\n",
    "    if re.match(r'^[A-Za-z]+, \\d{4}$', x):\n",
    "        return x\n",
    "\n",
    "    # Match \"X days ago\"\n",
    "    match_days = re.match(r'(\\d+)\\s+days?\\s+ago', x)\n",
    "    if match_days:\n",
    "        days_ago = int(match_days.group(1))\n",
    "        estimated_date = datetime.today() - timedelta(days=days_ago)\n",
    "        return estimated_date.strftime(\"%b, %Y\")\n",
    "\n",
    "    # Match \"X months ago\"\n",
    "    match_months = re.match(r'(\\d+)\\s+months?\\s+ago', x)\n",
    "    if match_months:\n",
    "        months_ago = int(match_months.group(1))\n",
    "        estimated_date = datetime.today() - timedelta(days=months_ago * 30)\n",
    "        return estimated_date.strftime(\"%b, %Y\")\n",
    "\n",
    "    # Match \"1 month ago\" (non-numeric)\n",
    "    if \"month ago\" in x:\n",
    "        estimated_date = datetime.today() - timedelta(days=30)\n",
    "        return estimated_date.strftime(\"%b, %Y\")\n",
    "\n",
    "    # Match \"1 day ago\" (non-numeric)\n",
    "    if \"day ago\" in x:\n",
    "        estimated_date = datetime.today() - timedelta(days=1)\n",
    "        return estimated_date.strftime(\"%b, %Y\")\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a0b17c-8d97-4627-83d2-b6de51698c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_monthly created with count of reviews per month.\n",
      "<bound method NDFrame.head of                   Product                Author Rating  \\\n",
      "Parsed_Date                                              \n",
      "2019-06-01   MarQ AC 2025    kiran kumar Boyina      3   \n",
      "2019-06-01   MarQ AC 2025            Harsh Garg    4.2   \n",
      "2019-06-01   MarQ AC 2025          Sourav Dutta      5   \n",
      "2019-06-01   MarQ AC 2025  Krishnendu Chowdhury      5   \n",
      "2019-06-01   MarQ AC 2025          Harjot Singh      5   \n",
      "...                   ...                   ...    ...   \n",
      "2025-06-01   MarQ AC 2025   Adv. Dr. Raj Sharma      3   \n",
      "2025-06-01   MarQ AC 2025        Dr.Bhargav Ray      3   \n",
      "2025-06-01   MarQ AC 2025           Arun Sharma      5   \n",
      "2025-06-01     Realme C61     Flipkart Customer      4   \n",
      "NaT            Realme C61            Ade Tharun      4   \n",
      "\n",
      "                                                        Review         Date  \\\n",
      "Parsed_Date                                                                   \n",
      "2019-06-01   good product from marq, this is my second marq...    Jun, 2019   \n",
      "2019-06-01   In 8hrs from 9 am to 5 pm .. its take 10 Unit ...    Jun, 2019   \n",
      "2019-06-01   But until we use it for 2 months it is early t...    Jun, 2019   \n",
      "2019-06-01   Best inverter AC in the 1 ton segment ,\\ninsta...    Jun, 2019   \n",
      "2019-06-01                                             nice ac    Jun, 2019   \n",
      "...                                                        ...          ...   \n",
      "2025-06-01   Product is good and cooling is good with such\\...  10 days ago   \n",
      "2025-06-01   Bogus....10x10 feet room is not getting cold ....  12 days ago   \n",
      "2025-06-01   After using a month product seems to perform\\n...  17 days ago   \n",
      "2025-06-01                                            its good   2 days ago   \n",
      "NaT                                        Poor in performance        Today   \n",
      "\n",
      "            date_review  \n",
      "Parsed_Date              \n",
      "2019-06-01    Jun, 2019  \n",
      "2019-06-01    Jun, 2019  \n",
      "2019-06-01    Jun, 2019  \n",
      "2019-06-01    Jun, 2019  \n",
      "2019-06-01    Jun, 2019  \n",
      "...                 ...  \n",
      "2025-06-01    Jun, 2025  \n",
      "2025-06-01    Jun, 2025  \n",
      "2025-06-01    Jun, 2025  \n",
      "2025-06-01    Jun, 2025  \n",
      "NaT             Unknown  \n",
      "\n",
      "[600 rows x 6 columns]>\n",
      "✅ Data saved to flipkart_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9912\\1115321381.py:10: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_monthly = df.resample('M').size().to_frame(name='review_count')\n"
     ]
    }
   ],
   "source": [
    "# Clean and convert dates\n",
    "df = pd.DataFrame(all_reviews)  # Create the DataFrame\n",
    "df[\"date_review\"] = df[\"Date\"].apply(fix_date)\n",
    "\n",
    "df['date_review'] = df['date_review'].apply(fix_date)\n",
    "df['Parsed_Date'] = pd.to_datetime(df['date_review'], format='%b, %Y', errors='coerce')\n",
    "df = df.set_index('Parsed_Date').sort_index()\n",
    "\n",
    "# ✅ Resample: count how many reviews per month (based on date only)\n",
    "df_monthly = df.resample('M').size().to_frame(name='review_count')\n",
    "print(\"✅ df_monthly created with count of reviews per month.\")\n",
    "print(df.head)\n",
    "df.to_csv(\"C:\\\\Users\\\\hp\\\\Desktop\\\\thesis_codes\\\\flipkart_reviews.csv\", index=False)# Save it\n",
    "print(\"✅ Data saved to flipkart_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c0442-17ad-4845-8f96-bf7bea9a9fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
